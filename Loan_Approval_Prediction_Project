{"cells":[{"cell_type":"markdown","metadata":{"id":"j9EZvHmRB2RD"},"source":["# üìå Loan Approval Prediction using Machine Learning\n","\n","üë§ **Author:** Emmanuel J. Generale  \n","üìÖ **Date:** June 2025  \n","üíª **Platform:** Google Collab\n","\n","## üìò Project Description\n","This project aims to develop a machine learning model that predicts whether a loan application should be approved based on the applicant‚Äôs personal and financial information. I will use algorithms such as Logistic Regression, Decision Trees, Random Forests, and Boosting techniques to apply what I learn on Andrew NG ML-Course. The dataset used is a real-world anonymized loan dataset from kaggle.\n","\n","## üéØ Project Objectives\n","\n","- Apply what I‚Äôve learned in Course 1 & 2 of Andrew Ng‚Äôs Machine Learning Specialization\n","\n","- Practice real-world data cleaning and preprocessing, including handling missing values and encoding categorical features\n","\n","- Build and evaluate multiple classification models, including Logistic Regression, Random Forest, and Gradient Boosting\n","\n","- Understand model performance through accuracy, feature importance, and model tuning\n","\n","- Explore feature importance\n","\n","- Perform hyperparameter tuning\n","\n","- Document and share the entire ML workflow for personal learning and portfolio\n","- Learn how to save and reuse machine learning models using joblib\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"nKc9vHUTDAWH","outputId":"dd40c18a-4015-43f5-ef3c-59fee414ec7c"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-c47e2877-933e-430f-8a17-f72725f60ddb\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-c47e2877-933e-430f-8a17-f72725f60ddb\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from google.colab import files\n","uploaded = files.upload()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHlP69XOCar3"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_csv('train_u6lujuX_CVtuZ9i.csv')\n","\n","# Displaying the first 5 rows to check contents\n","df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"FxnsvgJ5EXfa"},"source":["#Inspect the Dataset Structure#\n","\n","**Check the Basic Structure and Missing Values.**\n","\n","‚óè To know which columns need cleaning\n","\n","‚óè And also to understand the data types so to choose appropriate preprocessing steps.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TG72QZxEe7c"},"outputs":[],"source":["# Info about columns, data types, and non-null counts\n","df.info()\n","\n","# Count how many missing values per column\n","df.isnull().sum()\n"]},{"cell_type":"markdown","metadata":{"id":"UFb87IPEGloL"},"source":["**One thing I've learn is missing Values shouldn't be ignore because they can:**\n","\n","- Break certain models like logistic regression, Reduce accuracy, or Bias the results if not handled properly.\n","\n","\n","- These missing values means that some applicants didn‚Äôt fill out those fields. And we need to fill or fix them before training the model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aPErXJztMCFx"},"source":["#Data Cleaning & Preprocessing#\n","#üéØ Goal:#\n","\n","‚óè Handle missing values\n","\n","\n","‚óèPrepare data for modeling (clean, consistent, and usable)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fql1dNNTNgKy"},"source":["\n","**Fill Missing Values**:\n","Let‚Äôs handle the columns one by one.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfkIqQ59dx2S"},"outputs":[],"source":["# Fill missing values\n","\n","df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n","df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n","df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n","df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)\n","df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n","df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n","df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XYp5-xliOinj"},"source":["**Why do I use mode to fill missing values in categorical columns?**\n","\n","Since we have missing value for categorial, it's reasonable to assume it's the most common one.\n","**Example: if 80% are \"Male\", it's likely that a missing value is also \"Male\"**\n","\n"]},{"cell_type":"code","source":["df.drop('Loan_ID', axis=1, inplace=True)\n","#axis=1 column\n","#inplace=T to change original df"],"metadata":{"id":"xUa13sKLg38e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLDV0GuBUXKr"},"source":["**Numeric Column ‚Üí Fill with Median**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owVm9yOlUODI"},"outputs":[],"source":["# Fill LoanAmount with the median (less sensitive to outliers)\n","df['LoanAmount'].fillna(df['LoanAmount'].median(), inplace=True)\n","# Recheck for missing values\n","df.isnull().sum()\n"]},{"cell_type":"markdown","metadata":{"id":"lRZefH0CRVwI"},"source":["##Encode Categorical Variables##\n","\n","##üéØ Goal:##\n","\n","‚óè Machine learning models work with numbers, not text. So we need to convert text categories like \"Male\", \"Yes\", \"Graduate\" into numeric values.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I19pr5T0Wz12"},"source":["##Let‚Äôs list all categorical columns first:##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOtCu-vCVfkS"},"outputs":[],"source":["df.select_dtypes(include='object').columns"]},{"cell_type":"markdown","metadata":{"id":"N1POVXLYWRsc"},"source":["## We will encode Target Column\n","\n","Loan_Status is our target: it‚Äôs \"Y\" for approved, \"N\" for rejected.\n","\n","\n","**And also label Encode Binary Columns**:\n","These columns have only two unique values (binary):\n","\n","- Gender: Male / Female\n","\n","- Married: Yes / No\n","\n","- Education: Graduate / Not Graduate\n","\n","- Self_Employed: Yes / No\n","\n","\n","Lets Convert it:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIxSKc44XGvx"},"outputs":[],"source":["#Target column\n","df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n","\n","#Other Binary Columns\n","df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n","df['Married'] = df['Married'].map({'Yes': 1, 'No': 0})\n","df['Education'] = df['Education'].map({'Graduate': 1, 'Not Graduate': 0})\n","df['Self_Employed'] = df['Self_Employed'].map({'Yes': 1, 'No': 0})\n"]},{"cell_type":"markdown","metadata":{"id":"z1DkV4TMYvGl"},"source":["**One-Hot Encode Non-Binary Columns**:\n","\n","‚óè Dependents: ('0', '1', '2', '3+')\n","\n","‚óè Property_Area: ('Urban', 'Rural', 'Semiurban')\n","\n","We‚Äôll use pd.get_dummies() for one-hot encoding:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UUhFUutcQCe"},"outputs":[],"source":["df = pd.get_dummies(df, columns=['Dependents', 'Property_Area'], drop_first=True)\n","\n","df.info()\n"]},{"cell_type":"markdown","metadata":{"id":"2wYH8FzCcwgG"},"source":["‚òë Now all features become numeric, and we're now ready to start splitting data and modeling."]},{"cell_type":"markdown","source":["#üöÄ Let‚Äôs Now Proceed to Train-Test Split & Baseline Model (Logistic Regression)#\n","\n","##üéØ Goal:##\n","\n","‚óè Split your dataset into training and testing sets.\n","\n","‚óè Train a Logistic Regression model (your baseline classifier).\n","\n","‚óè Evaluate its basic performance.\n","\n","\n","\n"],"metadata":{"id":"lOZPKgcOiJX8"}},{"cell_type":"markdown","source":["**Split Into Training and Test Sets:**\n","\n","We will remove the Loan_Status column from the feature set (X).\n","\n","This prevents the model from \"cheating\" by seeing the answer during training.\n","\n"],"metadata":{"id":"H6Iui0h_jINM"}},{"cell_type":"code","source":["# Separate input features (X) and target variable (y)\n","X = df.drop('Loan_Status', axis=1)\n","y = df['Loan_Status']\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Split data: 80% training, 20% testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"LVhbaVxxjOcQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Test_size=0.2 ‚Üí 20% of the data is set aside for evaluation.*\n","\n","*Random_state=42 ‚Üí ensures reproducible results.*\n","\n"],"metadata":{"id":"vn0E9vzUkkAD"}},{"cell_type":"markdown","source":["**Scale the Features:**\n","\n","Since some features (like ApplicantIncome, LoanAmount) have very large values compared to others (like 0/1 binary features). We‚Äôll use StandardScaler to standardize all feature columns:\n","\n"],"metadata":{"id":"waz6TVJrmqNe"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","# Initialize scaler\n","scaler = StandardScaler()\n","\n","# Fit on training data and transform both train and test sets\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n"],"metadata":{"id":"ke9y2eiqluMw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train a Logistic Regression Model:**"],"metadata":{"id":"hx0grCRTksDg"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","# Create model with higher iteration limit\n","model = LogisticRegression(max_iter=1000)\n","\n","# Train on scaled data\n","model.fit(X_train_scaled, y_train)\n","\n","# Predict\n","y_pred = model.predict(X_test_scaled)\n"],"metadata":{"id":"0hF7_liMmAxA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Evaluate:**"],"metadata":{"id":"8cqccgkOnyXk"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Test Accuracy (scaled):\", accuracy)\n"],"metadata":{"id":"LvB0dL_inr_H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*The score of ~78.9% accuracy from our Logistic Regression baseline is a solid starting point ‚Äî especially for a real-world dataset. Now let‚Äôs aim to improve the performance using more powerful models.*"],"metadata":{"id":"c0WK_qwco1xL"}},{"cell_type":"markdown","source":["#Advanced Models ‚Äì Decision Tree, Random Forest, and Gradient Boosting#\n","\n","In this part, I will also apply what I learn on **Course 2 of Andrew Ng: ML Specialization.**\n","\n","These are tree-based models that often perform better than logistic regression, especially with non-linear patterns."],"metadata":{"id":"nh8eh0wapM2E"}},{"cell_type":"markdown","source":["**‚úÖ Decision Tree Classifier:**"],"metadata":{"id":"jwzExd_nqPxx"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","\n","# Initialize and train\n","dt_model = DecisionTreeClassifier(random_state=42)\n","dt_model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred_dt = dt_model.predict(X_test)\n","print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n"],"metadata":{"id":"hjXydCjgqVeY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**‚úÖ Random Forest Classifier:**"],"metadata":{"id":"gtXP2lOrqg4b"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Initialize and train\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred_rf = rf_model.predict(X_test)\n","print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"],"metadata":{"id":"1ZCEZhd5qurc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**‚úÖ Gradient Boosting:**"],"metadata":{"id":"ghTLfmObq579"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","# Initialize and train\n","gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","gb_model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred_gb = gb_model.predict(X_test)\n","print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n"],"metadata":{"id":"_KgPgqX-q48t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##üìä Model Performance Summary\n","| Model               | Accuracy (%)                                        |\n","| ------------------- | --------------------------------------------------- |\n","| Logistic Regression | **78.86%** ‚úÖ *(baseline model)*                     |\n","| Decision Tree       | 69.11% ‚ùå *(overfitting, weaker generalization)*     |\n","| Random Forest       | 77.24% ‚úÖ *(good, but slightly lower than baseline)* |\n","| Gradient Boosting   | **78.05%** ‚úÖ *(almost tied with baseline)*          |\n","\n","\n","##üß† Observations:\n","\n","‚óè ‚úÖ Logistic Regression performed best overall. This suggests the data might be linearly separable or the features work well with simple decision boundaries.\n","\n","‚óè ‚ùå Decision Tree underperformed ‚Äî this is common since a single tree tends to overfit.\n","\n","‚óè ‚úÖ Random Forest & Gradient Boosting did well but didn't outperform logistic regression by much maybe because:\n","\n","‚Ä¢ The dataset is small (only 614 rows)\n","\n","‚Ä¢ There‚Äôs not much non-linear pattern to exploit\n","\n","\n","\n"],"metadata":{"id":"RtiGJqTrsuup"}},{"cell_type":"markdown","source":["#üîπFeature Importance:\n","We will now understand which features contribute most to loan approval decisions.\n","\n","**I‚Äôll do this using:**\n","\n","Random Forest (good balance of accuracy + interpretability)\n","\n","\n","A simple bar chart to visualize\n","\n"],"metadata":{"id":"-AQXltGAvmTH"}},{"cell_type":"markdown","source":["**üìà Get Feature Importances from Random Forest:**"],"metadata":{"id":"ehVDKzMzyFZv"}},{"cell_type":"code","source":["# Get feature importances from trained Random Forest model\n","import pandas as pd\n","import numpy as np\n","\n","importances = rf_model.feature_importances_\n","feature_names = X.columns\n","\n","# Create a DataFrame for easy sorting\n","feat_imp_df = pd.DataFrame({\n","        'Feature': feature_names,\n","        'Importance': importances\n","        })\n","\n","# Sort by importance (descending)\n","feat_imp_df = feat_imp_df.sort_values(by='Importance', ascending=False)\n","print(feat_imp_df)\n"],"metadata":{"id":"0bL_QMwcxmof"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**üìà Visualize with Bar Plot:**"],"metadata":{"id":"s7uC4W6syLst"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(10, 6))\n","sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='viridis')\n","plt.title('Feature Importance (Random Forest)')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"xiG1cnhdyf4f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**‚úÖ Interpreting the Output:**\n","\n","**Feature Highest importance is Credit_History**, Meaning it has Strong influence on loan approval\n","\n","**Low importance is** Self_Employed, Dependents_2 meaning it has less impact."],"metadata":{"id":"_Z7zHp4XyvW5"}},{"cell_type":"markdown","source":["**Feature Importance from Gradient Boosting**\n","\n","Comparing Gradient Boosting feature importances to Random Forest‚Äôs will:\n","\n","* Confirm which features are consistently important\n","\n","* Spot any differences in how the models \"think\"\n","\n"],"metadata":{"id":"DaUM36yO2mjw"}},{"cell_type":"code","source":["# Extract feature importances from the Gradient Boosting model\n","gb_importances = gb_model.feature_importances_\n","\n","# Create DataFrame for comparison\n","gb_feat_imp_df = pd.DataFrame({\n","    'Feature': X.columns,\n","    'Importance': gb_importances\n","        }).sort_values(by='Importance', ascending=False)\n","\n","# Show the table\n","print(gb_feat_imp_df)\n"],"metadata":{"id":"RqKOfAto3TGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","sns.barplot(x='Importance', y='Feature', data=gb_feat_imp_df, palette='coolwarm')\n","plt.title('Feature Importance (Gradient Boosting)')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"sRmdAcNA5Xsl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**‚úÖ Merge Feature Importances:**\n"],"metadata":{"id":"V0_KR9GC52jl"}},{"cell_type":"code","source":["# Merge Random Forest and Gradient Boosting importances\n","merged_feat_imp = pd.DataFrame({\n","    'Feature': X.columns,\n","    'Random Forest': rf_model.feature_importances_,\n","    'Gradient Boosting': gb_model.feature_importances_\n","})\n","\n","# Sort by average importance\n","merged_feat_imp['Average'] = (merged_feat_imp['Random Forest'] + merged_feat_imp['Gradient Boosting']) / 2\n","merged_feat_imp = merged_feat_imp.sort_values(by='Average', ascending=False).drop(columns='Average')\n","\n","print(merged_feat_imp)\n"],"metadata":{"id":"ZHn2qyqx6fGc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**üìà Side-by-Side Bar Plot using Seaborn**"],"metadata":{"id":"38uwa1RY6vuR"}},{"cell_type":"code","source":["# Melt the DataFrame for easier plotting\n","melted = pd.melt(merged_feat_imp, id_vars='Feature',\n","            value_vars=['Random Forest', 'Gradient Boosting'],\n","            var_name='Model', value_name='Importance')\n","\n","plt.figure(figsize=(12, 8))\n","sns.barplot(x='Importance', y='Feature', hue='Model', data=melted)\n","plt.title('Feature Importance Comparison: Random Forest vs Gradient Boosting')\n","plt.xlabel('Importance')\n","plt.ylabel('Feature')\n","plt.legend(loc='lower right')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"uLz16DOp62FD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##üîç What I noticed:##\n","\n","**Top features consistent across both models** ‚úÖ\n","\n","**One model rely more on a specific feature than the other - The Gradient Boosting Model** ‚úÖ\n","\n","**There is features that Gradient Model almost ignores** ‚úÖ\n","\n"],"metadata":{"id":"CWsCIWJ87iSr"}},{"cell_type":"markdown","source":["#Hyperpqrameter Tuning (Random Forest) #\n","\n","**üéØ To improve our model performance.**\n","\n","We‚Äôll use RandomizedSearchCV first ‚Äî according to GPT it's faster and more practical than GridSearchCV when you're starting out, especially with limited resources (like running on a mobile or basic Colab).\n","\n","We'll tune the Random Forest model."],"metadata":{"id":"oSMkt5S_tFCG"}},{"cell_type":"markdown","source":["**Import the Tools & Define Hyperparameter Grid:**\n","\n","Here are some commonly tuned hyperparameters for Random Forest:\n","\n"],"metadata":{"id":"dNoHQiqstvHs"}},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Define the hyperparameter space\n","param_dist = {\n","    'n_estimators': [50, 100, 200, 300],\n","    'max_depth': [None, 5, 10, 20],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4],\n","    'max_features': ['auto', 'sqrt', 'log2']\n","                    }\n"],"metadata":{"id":"fplBNQDstqUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Perform Randomized Search:**"],"metadata":{"id":"zv5jbYYHueo6"}},{"cell_type":"code","source":["# Initialize the base model\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Initialize RandomizedSearchCV\n","random_search = RandomizedSearchCV(\n","    estimator=rf,\n","    param_distributions=param_dist,\n","    n_iter=20,           # Try 20 random combinations\n","    cv=5,                # 5-fold cross-validation\n","    scoring='accuracy',\n","    random_state=42,\n","    n_jobs=-1            # Use all CPU cores (if supported)\n",")\n","\n","# Fit the search on training data\n","random_search.fit(X_train, y_train)\n","\n","# Best model\n","best_rf = random_search.best_estimator_\n","print(\"Best Parameters:\", random_search.best_params_)\n"],"metadata":{"id":"VQ4EDeDpurQs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Evaluate Tuned Model:**"],"metadata":{"id":"PRIGjoICu54Q"}},{"cell_type":"code","source":["# Predict using best model\n","y_pred_best_rf = best_rf.predict(X_test)\n","\n","# Evaluate\n","print(\"Tuned Random Forest Accuracy:\", accuracy_score(y_test, y_pred_best_rf))\n"],"metadata":{"id":"vx3yK2uavCCv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Hyperparameter Tuning for Gradient Boosting##\n","let‚Äôs now tune the Gradient Boosting Classifier (GBM) using RandomizedSearchCV.\n","\n","\n"],"metadata":{"id":"oAneRg-MvpnB"}},{"cell_type":"markdown","source":["**Define Hyperparameter Grid:**"],"metadata":{"id":"uc6CP79jwBOP"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","param_grid_gbm = {\n","'n_estimators': [50, 100, 200, 300],\n","'learning_rate': [0.01, 0.05, 0.1, 0.2],\n","'max_depth': [3, 4, 5, 6],\n","'min_samples_split': [2, 5, 10],\n","'min_samples_leaf': [1, 2, 4],\n","'subsample': [0.6, 0.8, 1.0]\n","                          }\n","\n"],"metadata":{"id":"IR0uRL4kwPCh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Perform Randomized Search:**"],"metadata":{"id":"4odQ3KaHw3Z_"}},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","\n","# Initialize base GBM model\n","gbm = GradientBoostingClassifier(random_state=42)\n","\n","# Set up randomized search\n","random_search_gbm = RandomizedSearchCV(\n","    estimator=gbm,\n","    param_distributions=param_grid_gbm,\n","    n_iter=20,\n","    cv=5,\n","    scoring='accuracy',\n","    random_state=42,\n","    n_jobs=-1\n","        )\n","\n","# Fit search on training data\n","random_search_gbm.fit(X_train, y_train)\n","\n","\n","#Best model\n","best_gbm = random_search_gbm.best_estimator_\n","print(\"Best Parameters for GBM:\", random_search_gbm.best_params_)\n"],"metadata":{"id":"qJPyedXAw-Pl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Evaluate the Tuned GBM:**"],"metadata":{"id":"EHhz9o80xhwF"}},{"cell_type":"code","source":["y_pred_best_gbm = best_gbm.predict(X_test)\n","print(\"Tuned Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_best_gbm))\n"],"metadata":{"id":"JhaK0QsExssr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##üìä Same Accuracy: 78.86% (RF & GBM after tuning)##\n","\n","**üîç Observation:**\n","‚úÖ Both tuned models reached the same test accuracy - This means they both learned similar decision boundaries from our dataset\n","\n","‚ùåNo accuracy improvement over Logistic Regression - The baseline which is also ~78.9% was already very strong"],"metadata":{"id":"ZSM6ryiSzLf5"}},{"cell_type":"markdown","source":["##Save the Final Model:##"],"metadata":{"id":"mLlb6MXH1ffn"}},{"cell_type":"code","source":["import joblib\n","\n","# Save the trained Gradient Boosting model\n","joblib.dump(best_gbm, 'loan_approval_model.pkl')\n"],"metadata":{"id":"z-gX09z216vY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To load and use it later:\n","loaded_model = joblib.load('loan_approval_model.pkl')\n","\n","# Make predictions\n","loaded_model.predict(X_test)\n"],"metadata":{"id":"5Dqj4ddY2Oyd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","\n","# Rename the current notebook (default is always '/content/Untitled0.ipynb' or similar)\n","shutil.copy('Loan_Approval_Prediction_Project.ipynb', '/content/loan_approval_notebook.ipynb')\n"],"metadata":{"id":"ikUSoEviZroz"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPrN5Ast8EIt2Eog7e6oRo6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}